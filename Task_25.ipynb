{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LldFApI8ygN",
        "outputId": "6f5a0b28-6461-4c48-c054-f7d9d92b916c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        # Initialize the logistic regression model with a learning rate and number of iterations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        # Sigmoid function to map values to a range between 0 and 1\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def compute_cost(self, X, y):\n",
        "        # Compute the cost function (cross-entropy loss)\n",
        "        m = len(y)  # Number of training examples\n",
        "        predictions = self.sigmoid(np.dot(X, self.theta))  # Predicted probabilities\n",
        "        cost = -1/m * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))  # Cross-entropy loss\n",
        "        return cost\n",
        "\n",
        "    def gradient_descent(self, X, y):\n",
        "        # Perform gradient descent to optimize the model parameters\n",
        "        m = len(y)  # Number of training examples\n",
        "        for _ in range(self.num_iterations):\n",
        "            predictions = self.sigmoid(np.dot(X, self.theta))  # Predicted probabilities\n",
        "            gradients = np.dot(X.T, (predictions - y)) / m  # Gradient of the cost function\n",
        "            self.theta -= self.learning_rate * gradients  # Update weights\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Train the logistic regression model\n",
        "        self.theta = np.zeros(X.shape[1])  # Initializing weights to zeros\n",
        "        self.gradient_descent(X, y)  # Optimizing weights using gradient descent\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict class labels for given data\n",
        "        predictions = self.sigmoid(np.dot(X, self.theta))  # Predicted probabilities\n",
        "        return [1 if p >= 0.5 else 0 for p in predictions]  # Convert probabilities to binary labels\n",
        "\n",
        "# Example data (features and labels)\n",
        "X = np.array([[1, 2], [1, 3], [2, 3], [2, 4]])  # Feature matrix\n",
        "y = np.array([0, 0, 1, 1])  # Labels\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegression(learning_rate=0.1, num_iterations=1000)  # Initialize model\n",
        "model.fit(X, y)  # Train model on the data\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)  # Predict labels for the training data\n",
        "print(predictions)  # Output the predictions\n"
      ]
    }
  ]
}