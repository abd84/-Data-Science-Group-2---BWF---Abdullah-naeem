{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural language processing"
      ],
      "metadata": {
        "id": "aEqGNTrPhzuP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vs6yhq2-hvwZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MuAHM7-h4e6",
        "outputId": "263f4b69-aafd-4cb8-85f1-bdc48540cd5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'yours', 'above', 'other', 'before', \"she's\", 'but', 'yourself', 'have', 'its', \"you're\", 'as', 'isn', \"isn't\", 'up', 'further', 'being', 'him', 'wasn', 'once', \"doesn't\", 'weren', 'wouldn', \"you'll\", 'shan', \"needn't\", 'o', 'no', \"weren't\", 'they', 'our', \"hadn't\", 'after', 'd', 'at', 'doing', 't', 'out', 'such', 'whom', \"mustn't\", \"you've\", 'now', 'you', 'ma', 'or', 'haven', 'nor', 'more', 'are', 'through', 'too', 'into', \"it's\", 'an', 'i', 'how', 'there', \"you'd\", 'shouldn', 'because', 'myself', 'these', 'all', 'with', \"should've\", 'y', 'by', 'that', 'been', 'ain', 's', 'most', \"that'll\", 'from', 'very', 'was', 'what', 've', 'herself', 'did', 'only', 'is', 'my', 'll', 'then', \"wouldn't\", 'himself', 'if', 're', 'here', 'why', 'about', 'for', 'has', 'down', 'not', 'when', 'which', 'ours', 'm', \"won't\", 'he', 'during', 'against', 'aren', 'on', 'having', 'yourselves', 'don', 'so', 'who', 'can', 'over', 'me', 'your', 'own', 'any', 'under', 'it', 'this', 'below', 'mustn', 'same', \"haven't\", 'his', 'again', 'each', \"shan't\", 'the', \"wasn't\", 'itself', 'those', 'and', 'few', 'than', 'a', 'am', 'while', 'her', 'where', 'hasn', 'will', \"didn't\", 'won', 'didn', \"don't\", 'to', 'be', 'some', \"shouldn't\", 'hadn', 'had', 'doesn', 'ourselves', 'hers', 'theirs', 'just', 'should', \"couldn't\", \"hasn't\", 'we', 'do', 'in', 'off', 'needn', 'until', 'does', 'couldn', \"mightn't\", 'them', 'were', \"aren't\", 'she', 'both', 'mightn', 'between', 'their', 'themselves', 'of'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file data.txt\n",
        " \"üöÄ AI is transforming the world! #AI #Innovation üåç\",\n",
        "    \"Python 3.9 was released on October 5th, 2020. Check it out at https://www.python.org!\",\n",
        "    \"Bonjour! Comment √ßa va? üòä #French #Language\",\n",
        "    \"Visit our site at https://www.example.com for more details! #Tech #News\",\n",
        "    \"Our meeting is scheduled for 10:00 AM on 12/15/2024. Don't be late! üïí\",\n",
        "    \"@john_doe I totally agree with your thoughts on climate change. #Environment #Sustainability\",\n",
        "    \"Breaking News: Major earthquake hits San Francisco! üì¢ Stay tuned for updates. #BreakingNews\",\n",
        "    \"Happy Diwali! ü™î Let's celebrate with sweets and lights. #Festivals #India\",\n",
        "    \"This product is amazing!! üòç Highly recommend to everyone. #Reviews #Product\",\n",
        "    \"Hola! ¬øQu√© tal? Estoy aprendiendo espa√±ol. #Learning #Spanish\",\n",
        "    \"Just landed in NYC! üóΩ Time to explore the city. #Travel #NewYork\",\n",
        "    \"Today‚Äôs temperature is 23¬∞C, with a 60% chance of rain. ‚òî Stay safe! #Weather\",\n",
        "    \"I'm feeling lucky today! üçÄ Let's see how it goes. #Fortune #Luck\",\n",
        "    \"New blog post is live: 'How to use TensorFlow for deep learning' üî• Read it now: https://blog.example.com/deep-learning\",\n",
        "    \"Had a great workout session! üí™ #Fitness #Health\",\n",
        "    \"Big data is revolutionizing industries. Learn more at https://bigdata.example.com #BigData #Analytics\",\n",
        "    \"Hey @alice, did you finish the report? We need it by 5 PM. üìä #Work #Deadlines\",\n",
        "    \"Can't wait for the weekend! üéâ Plans? #TGIF #Weekend\",\n",
        "    \"My flight got delayed by 3 hours. üòì #TravelWoes #Airlines\",\n",
        "    \"Had an amazing time at the concert last night! üé§ #Music #Live\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnbdJkr_iJGr",
        "outputId": "1394f33a-5275-44cc-906e-1bc43e5791c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing function"
      ],
      "metadata": {
        "id": "3LsFZ4nriTds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtags (but keep the words)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    # Remove emojis and other non-word characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "oB-zXh8AiQ-2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.txt', 'r', encoding='utf-8') as file:\n",
        "    data = file.readlines()\n",
        "preprocessed_texts = [preprocess_text(text) for text in data]\n",
        "for i, text in enumerate(preprocessed_texts):\n",
        "    print(f\"Original: {data[i]}\")\n",
        "    print(f\"Preprocessed: {text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6HUaOwFiXEg",
        "outputId": "0eab4a90-28c4-4ce6-b19e-5dff9bd8533c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  \"üöÄ AI is transforming the world! #AI #Innovation üåç\",\n",
            "\n",
            "Preprocessed: ai transforming world ai innovation\n",
            "\n",
            "Original:     \"Python 3.9 was released on October 5th, 2020. Check it out at https://www.python.org!\",\n",
            "\n",
            "Preprocessed: python 39 released october 5th 2020 check\n",
            "\n",
            "Original:     \"Bonjour! Comment √ßa va? üòä #French #Language\",\n",
            "\n",
            "Preprocessed: bonjour comment √ßa va french language\n",
            "\n",
            "Original:     \"Visit our site at https://www.example.com for more details! #Tech #News\",\n",
            "\n",
            "Preprocessed: visit site details tech news\n",
            "\n",
            "Original:     \"Our meeting is scheduled for 10:00 AM on 12/15/2024. Don't be late! üïí\",\n",
            "\n",
            "Preprocessed: meeting scheduled 1000 12152024 dont late\n",
            "\n",
            "Original:     \"@john_doe I totally agree with your thoughts on climate change. #Environment #Sustainability\",\n",
            "\n",
            "Preprocessed: totally agree thoughts climate change environment sustainability\n",
            "\n",
            "Original:     \"Breaking News: Major earthquake hits San Francisco! üì¢ Stay tuned for updates. #BreakingNews\",\n",
            "\n",
            "Preprocessed: breaking news major earthquake hits san francisco stay tuned updates breakingnews\n",
            "\n",
            "Original:     \"Happy Diwali! ü™î Let's celebrate with sweets and lights. #Festivals #India\",\n",
            "\n",
            "Preprocessed: happy diwali lets celebrate sweets lights festivals india\n",
            "\n",
            "Original:     \"This product is amazing!! üòç Highly recommend to everyone. #Reviews #Product\",\n",
            "\n",
            "Preprocessed: product amazing highly recommend everyone reviews product\n",
            "\n",
            "Original:     \"Hola! ¬øQu√© tal? Estoy aprendiendo espa√±ol. #Learning #Spanish\",\n",
            "\n",
            "Preprocessed: hola qu√© tal estoy aprendiendo espa√±ol learning spanish\n",
            "\n",
            "Original:     \"Just landed in NYC! üóΩ Time to explore the city. #Travel #NewYork\",\n",
            "\n",
            "Preprocessed: landed nyc time explore city travel newyork\n",
            "\n",
            "Original:     \"Today‚Äôs temperature is 23¬∞C, with a 60% chance of rain. ‚òî Stay safe! #Weather\",\n",
            "\n",
            "Preprocessed: todays temperature 23c 60 chance rain stay safe weather\n",
            "\n",
            "Original:     \"I'm feeling lucky today! üçÄ Let's see how it goes. #Fortune #Luck\",\n",
            "\n",
            "Preprocessed: im feeling lucky today lets see goes fortune luck\n",
            "\n",
            "Original:     \"New blog post is live: 'How to use TensorFlow for deep learning' üî• Read it now: https://blog.example.com/deep-learning\",\n",
            "\n",
            "Preprocessed: new blog post live use tensorflow deep learning read\n",
            "\n",
            "Original:     \"Had a great workout session! üí™ #Fitness #Health\",\n",
            "\n",
            "Preprocessed: great workout session fitness health\n",
            "\n",
            "Original:     \"Big data is revolutionizing industries. Learn more at https://bigdata.example.com #BigData #Analytics\",\n",
            "\n",
            "Preprocessed: big data revolutionizing industries learn bigdata analytics\n",
            "\n",
            "Original:     \"Hey @alice, did you finish the report? We need it by 5 PM. üìä #Work #Deadlines\",\n",
            "\n",
            "Preprocessed: hey finish report need 5 pm work deadlines\n",
            "\n",
            "Original:     \"Can't wait for the weekend! üéâ Plans? #TGIF #Weekend\",\n",
            "\n",
            "Preprocessed: cant wait weekend plans tgif weekend\n",
            "\n",
            "Original:     \"My flight got delayed by 3 hours. üòì #TravelWoes #Airlines\",\n",
            "\n",
            "Preprocessed: flight got delayed 3 hours travelwoes airlines\n",
            "\n",
            "Original:     \"Had an amazing time at the concert last night! üé§ #Music #Live\"\n",
            "\n",
            "Preprocessed: amazing time concert last night music live\n",
            "\n"
          ]
        }
      ]
    }
  ]
}