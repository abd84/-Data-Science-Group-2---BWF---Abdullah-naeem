{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOOiDRFTwSNQ",
        "outputId": "ad751729-ce5c-49af-b38b-4a81932ef8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression MSE: 0.04514935249610696\n",
            "Logistic Regression Accuracy: 0.9541984732824428\n",
            "Random Forest Accuracy: 0.9580152671755725\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "Best parameters found by grid search: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "Grid Search Random Forest Accuracy: 0.9427480916030534\n",
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "\n",
        "# Load dataset (for example, Titanic dataset)\n",
        "from sklearn.datasets import fetch_openml\n",
        "titanic = fetch_openml(name='titanic', version=1, as_frame=True)\n",
        "df = titanic.frame\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Separate target variable\n",
        "X = df.drop(columns=['survived'])\n",
        "y = df['survived']\n",
        "\n",
        "# Ensure target variable is numeric\n",
        "y = pd.to_numeric(y, errors='coerce')\n",
        "\n",
        "# Impute missing values\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "X[numerical_cols] = numerical_imputer.fit_transform(X[numerical_cols])\n",
        "\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X[categorical_cols] = categorical_imputer.fit_transform(X[categorical_cols])\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "encoded_cols = encoder.fit_transform(X[categorical_cols])\n",
        "\n",
        "# Create a DataFrame with encoded columns\n",
        "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Concatenate the original numerical columns with the encoded categorical columns\n",
        "X = pd.concat([X[numerical_cols], encoded_df], axis=1)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Linear Regression model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate Linear Regression\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "print(f'Linear Regression MSE: {mse_lr}')\n",
        "\n",
        "# Train Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate Logistic Regression\n",
        "y_pred_logreg = logreg.predict(X_test)\n",
        "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
        "print(f'Logistic Regression Accuracy: {accuracy_logreg}')\n",
        "\n",
        "# Train Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate Random Forest\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f'Random Forest Accuracy: {accuracy_rf}')\n",
        "\n",
        "# Grid Search for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_features': ['sqrt'],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f'Best parameters found by grid search: {grid_search.best_params_}')\n",
        "best_rf_grid = grid_search.best_estimator_\n",
        "y_pred_grid = best_rf_grid.predict(X_test)\n",
        "accuracy_grid = accuracy_score(y_test, y_pred_grid)\n",
        "print(f'Grid Search Random Forest Accuracy: {accuracy_grid}')\n",
        "\n",
        "# Random Search for Random Forest\n",
        "param_dist = {\n",
        "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
        "    'max_features': ['sqrt'],\n",
        "    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=100, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(f'Best parameters found by random search: {random_search.best_params_}')\n",
        "best_rf_random = random_search.best_estimator_\n",
        "y_pred_random = best_rf_random.predict(X_test)\n",
        "accuracy_random = accuracy_score(y_test, y_pred_random)\n",
        "print(f'Random Search Random Forest Accuracy: {accuracy_random}')\n"
      ]
    }
  ]
}